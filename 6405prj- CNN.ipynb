{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce980848",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ed1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = 'imdb_nolabel.txt'\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "data_lines = load_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e92e3",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c5e311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\59158\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Change all words to lowercase\n",
    "# Remove non-alphabetic characters and numbers\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Apply tokenize\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Get the sentence after preprocessing\n",
    "cleaned_data = [clean_text(line) for line in data_lines]\n",
    "tokenized_data = [tokenize_text(line) for line in cleaned_data]\n",
    "tokenized_texts = [' '.join(tokens) for tokens in tokenized_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b46b0",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c74153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape： (1000, 3113)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert text data to TF-IDF feature matrix\n",
    "features_tfidf = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in tokenized_data])\n",
    "\n",
    "# Print TF-IDF feature matrix shape\n",
    "print(\"TF-IDF shape：\", features_tfidf.shape)\n",
    "\n",
    "\n",
    "def extract_last_digit(input_file):\n",
    "    try:\n",
    "        last_digits = []  # Stores a list of extracted end-of-line characters\n",
    "        with open(input_file, 'r') as f_input:\n",
    "            for line in f_input:\n",
    "                line = line.strip()  # Remove newlines and spaces at the end of lines\n",
    "                if line and (line.endswith('0') or line.endswith('1')):\n",
    "                    # Extract characters at end of line and convert to integer\n",
    "                    last_digits.append(int(line[-1]))  \n",
    "\n",
    "        # Convert extracted character list to NumPy array\n",
    "        last_digits_array = np.array(last_digits)\n",
    "        return last_digits_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error：{e}\")\n",
    "        return None\n",
    "\n",
    "input_file ='imdb_labelled.txt'\n",
    "labels = extract_last_digit(input_file)\n",
    "\n",
    "# Divide the data set into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_tfidf, labels, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f19a0",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2596f4f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "29/29 [==============================] - 3s 81ms/step - loss: 0.6352 - accuracy: 0.6422\n",
      "Epoch 2/6\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 0.3991 - accuracy: 0.8444\n",
      "Epoch 3/6\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 0.2447 - accuracy: 0.9156\n",
      "Epoch 4/6\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 0.1421 - accuracy: 0.9644\n",
      "Epoch 5/6\n",
      "29/29 [==============================] - 2s 77ms/step - loss: 0.0734 - accuracy: 0.9878\n",
      "Epoch 6/6\n",
      "29/29 [==============================] - 2s 73ms/step - loss: 0.0361 - accuracy: 0.9944\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "Accuracy: 0.77\n",
      "F1 Score: 0.78\n",
      "Precision: 0.85\n",
      "Recall: 0.71\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "X_train_dense = X_train.toarray()  \n",
    "X_test_dense = X_test.toarray()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((X_train_dense.shape[1], 1), input_shape=(X_train_dense.shape[1],)))  # Reshape for 1D convolution\n",
    "model.add(Conv1D(64, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Or 'softmax'\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_dense, y_train, epochs=6, batch_size=32)\n",
    "\n",
    "y_pred = (model.predict(X_test_dense) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='binary')  # Use binary for binary classification\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"F1 Score: {:.2f}\".format(f1))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41ca01",
   "metadata": {},
   "source": [
    "Completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f98fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fa801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
